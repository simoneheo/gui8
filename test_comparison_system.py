#!/usr/bin/env python3\n\"\"\"\nTest script for the comparison system.\n\nThis script tests the comparison methods and registry system to ensure\neverything is working correctly.\n\"\"\"\n\nimport numpy as np\nimport sys\nimport os\n\n# Add the current directory to Python path\nsys.path.insert(0, os.getcwd())\n\ndef test_comparison_registry():\n    \"\"\"Test the comparison registry system.\"\"\"\n    print(\"=== Testing Comparison Registry ===\")\n    \n    try:\n        from comparison import ComparisonRegistry\n        \n        # Initialize registry\n        print(\"Initializing comparison registry...\")\n        ComparisonRegistry.initialize()\n        \n        # Get registry stats\n        stats = ComparisonRegistry.get_registry_stats()\n        print(f\"Registry stats: {stats}\")\n        \n        # List all methods\n        methods = ComparisonRegistry.get_all_methods()\n        print(f\"Available methods: {methods}\")\n        \n        # List categories\n        categories = ComparisonRegistry.get_all_categories()\n        print(f\"Available categories: {categories}\")\n        \n        # Test method info\n        for method in methods:\n            info = ComparisonRegistry.get_method_info(method)\n            print(f\"\\n{method}:\")\n            print(f\"  Description: {info['description']}\")\n            print(f\"  Category: {info['category']}\")\n            print(f\"  Parameters: {list(info['parameters'].keys())}\")\n        \n        return True\n        \n    except Exception as e:\n        print(f\"Registry test failed: {e}\")\n        import traceback\n        traceback.print_exc()\n        return False\n\ndef test_correlation_comparison():\n    \"\"\"Test the correlation comparison method.\"\"\"\n    print(\"\\n=== Testing Correlation Comparison ===\")\n    \n    try:\n        from comparison import ComparisonRegistry\n        \n        # Generate test data\n        np.random.seed(42)\n        n = 100\n        ref_data = np.random.normal(0, 1, n)\n        test_data = 0.8 * ref_data + 0.2 * np.random.normal(0, 1, n)  # Correlated data\n        \n        print(f\"Generated test data: n={n}, correlation should be ~0.8\")\n        \n        # Create comparison method\n        method = ComparisonRegistry.create_method(\"Correlation Analysis\")\n        if method is None:\n            print(\"Failed to create correlation method\")\n            return False\n        \n        print(f\"Created method: {method}\")\n        \n        # Perform comparison\n        results = method.compare(ref_data, test_data)\n        \n        print(\"\\nResults:\")\n        print(f\"  Sample size: {results['n_samples']}\")\n        print(f\"  Valid ratio: {results['valid_ratio']:.3f}\")\n        \n        # Print correlations\n        if 'correlations' in results:\n            print(\"\\n  Correlations:\")\n            for corr_type, corr_data in results['correlations'].items():\n                if 'coefficient' in corr_data:\n                    print(f\"    {corr_type}: {corr_data['coefficient']:.4f} (p={corr_data.get('p_value', 'N/A')})\")\n        \n        # Print interpretations\n        if 'interpretation' in results:\n            print(\"\\n  Interpretations:\")\n            for key, interp in results['interpretation'].items():\n                print(f\"    {key}: {interp}\")\n        \n        return True\n        \n    except Exception as e:\n        print(f\"Correlation test failed: {e}\")\n        import traceback\n        traceback.print_exc()\n        return False\n\ndef test_bland_altman_comparison():\n    \"\"\"Test the Bland-Altman comparison method.\"\"\"\n    print(\"\\n=== Testing Bland-Altman Comparison ===\")\n    \n    try:\n        from comparison import ComparisonRegistry\n        \n        # Generate test data with some bias\n        np.random.seed(42)\n        n = 50\n        ref_data = np.random.normal(10, 2, n)\n        test_data = ref_data + 1.5 + np.random.normal(0, 0.5, n)  # Systematic bias + random error\n        \n        print(f\"Generated test data: n={n}, with systematic bias of 1.5\")\n        \n        # Create comparison method\n        method = ComparisonRegistry.create_method(\"Bland-Altman Analysis\")\n        if method is None:\n            print(\"Failed to create Bland-Altman method\")\n            return False\n        \n        print(f\"Created method: {method}\")\n        \n        # Perform comparison\n        results = method.compare(ref_data, test_data)\n        \n        print(\"\\nResults:\")\n        print(f\"  Sample size: {results['n_samples']}\")\n        \n        # Print bias analysis\n        if 'bias_analysis' in results:\n            bias = results['bias_analysis']\n            print(f\"\\n  Bias Analysis:\")\n            print(f\"    Bias: {bias['bias']:.4f}\")\n            print(f\"    Significant: {bias['significant']}\")\n            print(f\"    P-value: {bias['p_value']:.6f}\")\n        \n        # Print limits of agreement\n        if 'limits_of_agreement' in results:\n            limits = results['limits_of_agreement']\n            print(f\"\\n  Limits of Agreement:\")\n            print(f\"    Lower: {limits['lower_limit']:.4f}\")\n            print(f\"    Upper: {limits['upper_limit']:.4f}\")\n            print(f\"    Width: {limits['width']:.4f}\")\n        \n        # Print interpretations\n        if 'interpretation' in results:\n            print(\"\\n  Interpretations:\")\n            for key, interp in results['interpretation'].items():\n                print(f\"    {key}: {interp}\")\n        \n        return True\n        \n    except Exception as e:\n        print(f\"Bland-Altman test failed: {e}\")\n        import traceback\n        traceback.print_exc()\n        return False\n\ndef test_statistical_comparison():\n    \"\"\"Test the statistical comparison method.\"\"\"\n    print(\"\\n=== Testing Statistical Comparison ===\")\n    \n    try:\n        from comparison import ComparisonRegistry\n        \n        # Generate test data with different means\n        np.random.seed(42)\n        n1, n2 = 30, 35\n        ref_data = np.random.normal(5, 1.5, n1)\n        test_data = np.random.normal(6, 1.8, n2)  # Different mean and variance\n        \n        print(f\"Generated test data: ref n={n1} (mean~5), test n={n2} (mean~6)\")\n        \n        # Create comparison method\n        method = ComparisonRegistry.create_method(\"Statistical Tests\")\n        if method is None:\n            print(\"Failed to create statistical method\")\n            return False\n        \n        print(f\"Created method: {method}\")\n        \n        # Perform comparison\n        results = method.compare(ref_data, test_data)\n        \n        print(\"\\nResults:\")\n        print(f\"  Reference n: {results['n_ref']}\")\n        print(f\"  Test n: {results['n_test']}\")\n        \n        # Print descriptive statistics\n        if 'descriptive_statistics' in results:\n            desc = results['descriptive_statistics']\n            print(f\"\\n  Descriptive Statistics:\")\n            print(f\"    Ref mean: {desc['ref_mean']:.4f} ¬± {desc['ref_std']:.4f}\")\n            print(f\"    Test mean: {desc['test_mean']:.4f} ¬± {desc['test_std']:.4f}\")\n        \n        # Print assumption tests\n        if 'assumption_tests' in results:\n            print(f\"\\n  Assumption Tests:\")\n            assumptions = results['assumption_tests']\n            if 'normality' in assumptions:\n                norm = assumptions['normality']\n                print(f\"    Normality: ref={norm['ref_normal']}, test={norm['test_normal']}\")\n            if 'equal_variance' in assumptions:\n                var = assumptions['equal_variance']\n                print(f\"    Equal variance: {var['equal_variances']}\")\n        \n        # Print statistical tests\n        if 'statistical_tests' in results:\n            print(f\"\\n  Statistical Tests:\")\n            for test_name, test_result in results['statistical_tests'].items():\n                if isinstance(test_result, dict) and 'p_value' in test_result:\n                    print(f\"    {test_name}: p={test_result['p_value']:.6f}, significant={test_result['significant']}\")\n        \n        # Print effect sizes\n        if 'effect_sizes' in results:\n            print(f\"\\n  Effect Sizes:\")\n            effects = results['effect_sizes']\n            for effect_name, effect_value in effects.items():\n                if isinstance(effect_value, (int, float)):\n                    print(f\"    {effect_name}: {effect_value:.4f}\")\n                else:\n                    print(f\"    {effect_name}: {effect_value}\")\n        \n        # Print interpretations\n        if 'interpretation' in results:\n            print(\"\\n  Interpretations:\")\n            for key, interp in results['interpretation'].items():\n                print(f\"    {key}: {interp}\")\n        \n        return True\n        \n    except Exception as e:\n        print(f\"Statistical test failed: {e}\")\n        import traceback\n        traceback.print_exc()\n        return False\n\ndef main():\n    \"\"\"Run all tests.\"\"\"\n    print(\"Testing Comparison System\\n\")\n    \n    tests = [\n        test_comparison_registry,\n        test_correlation_comparison,\n        test_bland_altman_comparison,\n        test_statistical_comparison\n    ]\n    \n    results = []\n    for test_func in tests:\n        try:\n            result = test_func()\n            results.append(result)\n        except Exception as e:\n            print(f\"Test {test_func.__name__} failed with exception: {e}\")\n            results.append(False)\n    \n    # Summary\n    print(\"\\n=== Test Summary ===\")\n    passed = sum(results)\n    total = len(results)\n    print(f\"Passed: {passed}/{total}\")\n    \n    if passed == total:\n        print(\"üéâ All tests passed! Comparison system is working correctly.\")\n    else:\n        print(\"‚ùå Some tests failed. Check the output above for details.\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main() 